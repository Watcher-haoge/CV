{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、什么是PyTorch\n",
    "PyTorch 是一个基于 Python 的科学计算包，有两大用途：\n",
    "\n",
    "- 替代 NumPy 以使用 GPU 和其他加速器的强大功能。\n",
    "\n",
    "- 一个用于实现神经网络的自动微分库。\n",
    "\n",
    "# 二、本教程的目标：\n",
    "- 高层次理解 PyTorch 的 Tensor 库和神经网络。\n",
    "\n",
    "- 训练一个小型神经网络来对图像进行分类\n",
    "\n",
    "【注：确保您安装或者选择了torch和torchvision软件包。】"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、Tensors(张量)\n",
    "张量（Tensors）是一种特殊的数据结构，与数组和矩阵非常相似。在 PyTorch 中，我们使用张量对模型的输入和输出以及模型的参数进行编码。\n",
    "\n",
    "张量（Tensors）类似于' NumPy' 的 ndarray，不同之处在于张量可以在 GPU 或其他专用硬件上运行以加速计算。如果您熟悉 ndarrays，那么您将熟悉 Tensor API。如果没有，请按照此快速 API 演练进行操作。\n",
    "\n",
    "后续需要使用到的库如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 张量（Tensors）的初始化\n",
    "张量（Tensors）可以通过多种方式初始化。看看下面的例子："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 直接从数据初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "data = [[1, 2], [3,4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 从Numpy的array初始化\n",
    "张量（Tensors）可以从 NumPy 数组创建（反之亦然 - 请参阅[Bridge with NumPy](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#bridge-to-np-label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np_nrray:\n",
      " [[1 2]\n",
      " [3 4]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]], dtype=torch.int32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_array = np.array(data)\n",
    "print('np_nrray:\\n', np_array)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "x_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 从另外一个张量（Tensors）获取\n",
    "**注意**：除非明确覆盖，否则新张量保留参数张量的属性（形状、数据类型）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全1的张量: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "随机张量: \n",
      " tensor([[0.9234, 0.4061],\n",
      "        [0.4106, 0.0377]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"全1的张量: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(f\"随机张量: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 使用随机值或常量值\n",
    "shape是张量维度的元组。在下面的函数中，它决定了输出张量的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机张量: \n",
      " tensor([[0.7258, 0.9163, 0.2002],\n",
      "        [0.1320, 0.2362, 0.5964]]) \n",
      "\n",
      "全1张量: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "零张量: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2, 3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"随机张量: \\n {rand_tensor} \\n\")\n",
    "print(f\"全1张量: \\n {ones_tensor} \\n\")\n",
    "print(f\"零张量: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 张量（Tensors）的属性\n",
    "张量属性描述了它们的形状、数据类型和存储它们的设备（比如CPU还是GPU）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 张量（Tensors）的操作\n",
    "[这里](https://pytorch.org/docs/stable/torch.html)全面介绍了100 多种张量运算，包括转置、索引、切片、数学运算、线性代数、随机采样等 。\n",
    "它们中的每一个都可以在 GPU 上运行（速度通常比在 CPU 上更高），其实全文中，只有这里做了一下GPU的加载，后续没有用到GPU，所以下面一段代码，只是尝试一下，加载不出来没有关系。\n",
    "当然如果你不差钱，请转到运行的时候选择合适的含有GPU的套餐：（**要选取含有Pytorch的环境，否则没有Torch包**）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device tensor is stored on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# We move our tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "  tensor = tensor.to('cuda')\n",
    "  print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 numpy标准操作：索引和切片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 连接张量\n",
    "您可以`torch.cat`用来连接沿给定维度的一系列张量。另请参阅t[orch.stack](https://pytorch.org/docs/stable/generated/torch.stack.html)，另一个加入 op 的张量，与`torch.cat`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)  # dim=0 代表行，dim=1代表列\n",
    "t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 乘法张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.mul(tensor) \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor * tensor \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# This computes the element-wise product\n",
    "print(f\"tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor * tensor \\n {tensor * tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.matmul(tensor.T) \n",
      " tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]]) \n",
      "\n",
      "tensor @ tensor.T) \n",
      " tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "print(f'tensor.matmul(tensor.T) \\n {tensor.matmul(tensor.T)} \\n')\n",
    "print(f'tensor @ tensor.T) \\n {tensor @ tensor.T}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4 本地化操作\n",
    "\n",
    "具有`_`后缀的操作是本地化操作。例如：`x.copy_(y), x.t_()`, 将改变x。\n",
    "**注意**：就地操作节省了一些内存，但在计算导数时可能会出现问题，因为会立即丢失历史记录。因此，不鼓励使用它们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor, '\\n')\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 附加章节 使用`Numpy`桥接（可选看）\n",
    "**CPU 和 NumPy 数组上的张量可以共享它们的底层内存位置，改变一个将改变另一个。**\n",
    "\n",
    "### 3.4.1 张量（Tensors）到Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: \n",
      " tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "n: \n",
      " [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "t = torch.zeros((3,2))\n",
    "print(f't: \\n {t}')\n",
    "n = t.numpy()\n",
    "print(f'n: \\n {n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: \n",
      " tensor([[5., 5.],\n",
      "        [5., 5.],\n",
      "        [5., 5.]])\n",
      "n: \n",
      " [[5. 5.]\n",
      " [5. 5.]\n",
      " [5. 5.]]\n"
     ]
    }
   ],
   "source": [
    "t.add_(5)\n",
    "print(f't: \\n {t}')\n",
    "print(f'n: \\n {n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 Numpy到张量（Tensors）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.zeros((3,4))\n",
    "t = torch.from_numpy(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: \n",
      " tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]], dtype=torch.float64)\n",
      "n: \n",
      " [[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "np.add(n, 1, out=n)\n",
    "print(f\"t: \\n {t}\")\n",
    "print(f\"n: \\n {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、初步学习————torch.autograd\n",
    "`torch.autograd`是 PyTorch 的自动微分引擎，为神经网络训练提供动力。在本节中，您将从概念上了解 autograd 如何帮助神经网络训练。\n",
    "## 4.1 背景\n",
    "神经网络 (NN) 是对某些输入数据执行的嵌套函数的集合。这些函数由参数 （由权重和偏差组成）定义，这些参数在 PyTorch 中存储在张量（Tensors）中。\n",
    "\n",
    "训练 NN 分两步进行：\n",
    "\n",
    "1. **前向传播**：在前向传播中，神经网络对正确的输出做出最好的猜测。它通过它的每个函数运行输入数据来做出这个猜测。\n",
    "2. **反向传播**：在反向传播中，神经网络根据其猜测中的误差成比例地调整其参数。它通过从输出向后遍历，收集关于函数参数（梯度）的误差导数，并使用梯度下降优化参数来实现这一点。\n",
    "\n",
    "## 4.2 Pytorch的使用\n",
    "让我们看一下单个训练步骤。在这个例子中，我们从`torchvision`. 我们创建一个随机数据张量来表示具有 3 个通道、高度和宽度为 64 的单个图像，并将其对应的`label`初始化为一些随机值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18368\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\18368\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\18368/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [02:57<00:00, 263kB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "model = torchvision.models.resnet18(pretrained=True) # 使用 resnet18的模块，不懂没关系，我们重要的是学会整套“游泳动作”\n",
    "data = torch.rand(1,3,64,64) # [batch, channel, height, width]，表示batch_size=1(灰度图),3通道，图像尺寸64x64 的一个Tensor\n",
    "labels = torch.rand(1, 1000) # 表示1维的Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们通过模型的每一层运行输入数据以进行预测。这是**前传**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(data) # forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用模型的预测和相应的标签来计算误差 ( `loss`)。下一步是通过网络`反向传播`这个误差。当我们调用`.backward()`误差张量时，反向传播就开始了。`Autograd` 然后计算每个模型参数的梯度并将其存储在参数的`.grad`属性中。\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (prediction - labels).sum()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们加载一个优化器，在这种情况下，SGD 的学习率为 0.01，动量为 0.9。我们在优化器中注册模型的所有参数。SGD：随机梯度下降法。具体介绍见[torch.optim.SGD](https://blog.csdn.net/qq_34690929/article/details/79932416)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们调用`.step()`启动梯度下降。优化器通过存储在 中的梯度来调整每个参数`.grad`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.step() #gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时，您已拥有训练神经网络所需的一切。以下部分详细介绍了 `autograd` 的工作原理 - 随意跳过它们。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 `Autograd`中的差异化\n",
    "我们来看看如何`autograd`收集梯度。我们创建了两个张量a和b使用 `requires_grad=True`。这表明`autograd`应该跟踪对它们的每个操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor((2.,3.), requires_grad=True)\n",
    "b = torch.tensor((6.,4.), requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们从a和b中，创建另外一个张量（Tensor）Q：\n",
    "$Q = 3a^3 - b^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 3*a**3 - b**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们假设a和b都是神经网络的参数，并且Q是误差。在神经网络训练中，我们想将误差参数梯度化，举个例子：\n",
    "$\\frac{\\partial Q}{\\partial a} = 9a^2$\n",
    "$\\frac{\\partial Q}{\\partial b} = -2b$\n",
    "当我们对Q后向传播`.backward()`，`autograd` 计算这些梯度并将它们存储在各自的张量`.grad`属性中。\n",
    "我们需要显式地传入一个gradient参数，Q.backward()因为它是一个向量。 gradient是与 形状相同的张量Q，它表示 Q wrt 本身的梯度，即\n",
    "$\\frac{dQ}{dQ} = 1$\n",
    "\n",
    "等效地，我们也可以将 Q 聚合为一个标量并隐式地向后调用，例如`Q.sum()`，`.backward()`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度现在沉积在`a.grad`和`b.grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if collected gradients are correct\n",
    "print(9*a**2 == a.grad)\n",
    "print(-2*b == b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 计算图\n",
    "从概念上讲，`autograd` 在由Function 对象组成的有向无环图 (DAG) 中保存数据（张量）和所有已执行操作（以及由此产生的新张量）的记录 。在这个 DAG 中，叶子是输入张量，根是输出张量。通过从根到叶跟踪此图，您可以使用链式法则自动计算梯度。\n",
    "在前向传递中，autograd 同时做两件事：\n",
    "- 运行请求的操作来计算结果张量，和\n",
    "- 在 DAG 中维护操作的梯度函数。\n",
    "当`.backward()`在DAG根上调用时，反向传递开始。`autograd`开始做如下事情：\n",
    "- 计算每个梯度`.grad_fn`；\n",
    "- 将他们累计在各自的张量`.grad`属性中，并且\n",
    "- 使用链式法则，一直传播到叶张量。\n",
    "下面是我们示例中 DAG 的可视化表示。在图中，箭头指向前向传递的方向。节点代表前向传递中每个操作的后向函数。蓝色的叶子节点代表我们的叶子张量a和b。\n",
    "![](https://www.hualigs.cn/image/618249953ce89.jpg)\n",
    "\n",
    "> PyTorch 中 的DAG 是动态的 需要注意的重要一点是，该图是从头开始重新创建的；每次 .backward()调用后，autograd 开始填充新图形。这正是允许您在模型中使用控制流语句的原因；如果需要，您可以在每次迭代时更改形状、大小和操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 从DAG中排除\n",
    "\n",
    "`torch.autograd`跟踪所有张量的操作，这些张量的` requires_grad`标志设置为True. 对于不需要梯度的张量，设置此属性以False将其从梯度计算 DAG 中排除。\n",
    "\n",
    "即使只有一个输入张量具有 ，操作的输出张量也需要梯度`requires_grad=True`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5, 5)\n",
    "y = torch.rand(5, 5)\n",
    "z = torch.rand((5, 5), requires_grad=True)\n",
    "a = x + y\n",
    "print(f'张量a存在梯度吗？：{a.requires_grad}')\n",
    "b = x + z\n",
    "print(f'张量b存在梯度吗？：{b.requires_grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 NN 中，不计算梯度的参数通常称为冻结参数。如果您事先知道不需要这些参数的梯度，那么“冻结”模型的一部分很有用（这通过减少 autograd 计算提供了一些性能优势）。\n",
    "从 DAG 中排除很重要的另一个常见用例是对预训练网络进行[微调](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html)\n",
    "在微调中，我们冻结了大部分模型，通常只修改分类器层以对新标签进行预测。让我们通过一个小例子来演示这一点。和以前一样，我们加载一个预训练的 `resnet18` 模型，并冻结所有参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "# 冻结所有的梯度参数\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们要在具有 10 个标签的新数据集上微调模型。在 `resnet` 中，分类器是最后一个线性层`model.fc`。我们可以简单地用一个新的线性层（默认情况下未冻结）替换它作为我们的分类器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(512, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在模型中除`model.fc`的参数外的所有参数model.fc都被冻结，计算梯度的唯一参数是`model.fc`的权重和偏差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize only the classifier\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请注意，尽管我们在优化器中注册了所有参数，但计算梯度（因此在梯度下降中更新）的唯一参数是分类器的权重和偏差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 五、神经网络（Neural networks）\n",
    "可以使用该`torch.nn`包构建神经网络。\n",
    "现在您已经了解了`autograd,nn`取决于 `autograd`定义模型并区分它们。一个`nn.Module`包含层，以及一个`forward(input)`返回`output`。\n",
    "例如，以下是对数字图像的分类网络：\n",
    "[图像的分类网络](https://www.hualigs.cn/image/618254dce3f4d.jpg)\n",
    "这是一个简单的前馈网络。他接收输入，将其一层接一层的馈入，然后最终给出输出。\n",
    "一个典型的神经网络训练过程如下：\n",
    "- 定义具有一些可学习参数（或权重）的神经网络。\n",
    "- 迭代输入数据集。\n",
    "- 通过网络处理输入。\n",
    "- 计算损失（距离函数）。\n",
    "- 将梯度传播回网络参数。\n",
    "- 更新网络的权重，通常使用简单的更新规则：`weight = weight - learning_rate * gradient`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 定义网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__() # 将nn.Module的属性继承到self中。\n",
    "        # 1个输入通道，6个输出通道，,5x5的平方卷积\n",
    "        # 核心\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5) # 对由多个输入平面组成的输入信号进行二维卷积，卷积方式是：1个输入图像通道，产生6个输出通道，,卷积核的尺寸为5\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5) # 6个输入图像通道，产生16个输出图像通道，卷积核的尺寸为5\n",
    "        # 全连接层\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 最大池化为 (2, 2)的窗口\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2 ,2)) \n",
    "        # 如果尺寸是平方，你也可以用符号函数\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您只需要定义forward函数，backward 函数（计算梯度的地方）就会使用autograd. 您可以在forward函数中使用任何张量操作。\n",
    "模型的可学习参数由`net.parameters()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们尝试一个随机的 32x32 输入。注意：该网络 (LeNet) 的预期输入大小为 32x32。要在 MNIST 数据集上使用此网络，请将数据集中的图像大小调整为 32x32。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用随机梯度将所有参数和反向传播的梯度缓冲区归零："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**知识点**\n",
    "\n",
    "`torch.nn`仅支持小批量。整个`torch.nn`支持小批量样本的输入，而不支持单个样本。\n",
    "例如，`nn.Conv2d`将采用 4D 张量 。`nSamples x nChannels x Height x Width`\n",
    "\n",
    "如果您只有一个样本，只需使用`input.unsqueeze(0)`添加一个伪批次维度即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 损失函数\n",
    "> 损失函数：采用（输出、目标）对输入，并计算一个值，该值估计输出与目标的距离。\n",
    "nn包有几个不同的`损失函数`。如`nn.MSELoss`，其他损失函数[在这](https://pytorch.org/docs/stable/nn.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)\n",
    "target = target.view(1, -1)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，如果您loss向后移动，使用其`.grad_fn`属性，您将看到如下的计算图：\n",
    "> input -> conv2d -> relu ->maxpool2d -> conv2d ->relu ->maxpool2d ->flatten -> linear -> relu -> linear -> relu -> linear -> MSELoss -> loss\n",
    "\n",
    "因此，当我们调用`loss.backward()`，整个图根据神经网络参数进行微分，并且图中所有的`requires_grad=True`具有`.grad`张量都将随梯度累计其张量。\n",
    "\n",
    "为了说明问题，直接倒退几步：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss.grad_fn) # 倒退到MSELossoss\n",
    "print(loss.grad_fn.next_functions[0][0])   # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 反向传播\n",
    "为了反向传播错误，我们所要做的就是`loss.backward()`。但是要清除现有的梯度，否则新的梯度将会累积到现有的梯度中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "\n",
    "print(f'反向传播前：核心层的偏差：')\n",
    "print(net.conv1.bias.grad)\n",
    "loss.backward()\n",
    "print(f'反向传播后：核心层的偏差：')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 更新权重\n",
    "实践中使用的最简单的更新规则是随机梯度下降(SGD):\n",
    "$weight = weight - learning_rate * gradient$\n",
    "我们可以用简单的Python代码来实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然而，当您使用神经网络时，您希望使用各种不同的更新规则，例如 SGD、Nesterov-SGD、Adam、RMSProp 等。为了实现这一点，我们构建了一个小包：`torch.optim`它实现了所有这些方法。使用它非常简单："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# 创建你的optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# 进入训练循环中\n",
    "optimizer.zero_grad()\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 六、实战篇————图像识别分类器\n",
    "**首先恭喜你能够坚持看到这里，虽然你可能和笔者一样有很多东西目前没有搞明白，但是没关系，因为咱们现在已经跨过了深度学习入门的这道门槛。咱们会在后面的星辰大海中继续学习、复习和巩固**。\n",
    "\n",
    "您已经了解了如何定义神经网络、计算损失和更新网络权重。\n",
    "\n",
    "现在你可能会想，\n",
    "\n",
    "**数据呢**\n",
    "\n",
    "通常，当您必须处理图像、文本、音频或视频数据时，您可以使用将数据加载到 `numpy` 数组中的标准 python 包。然后，您可以将此数组转换为`torch.*Tensor`。\n",
    "\n",
    "> - 对于图像，Pillow、OpenCV 等包很有用\n",
    "> - 对于音频，scipy 和 librosa 等软件包\n",
    "> - 对于文本，基于原始 Python 或 Cython 的加载，或 NLTK 和 SpaCy 都很有用\n",
    "\n",
    "特别是对于视觉，我们创建了一个名为 的包 `torchvision`，它具有用于常见数据集（如 `ImageNet`、`CIFAR10`、`MNIST` 等）的数据加载器和用于图像的数据转换器，即，` torchvision.datasets`和`torch.utils.data.DataLoader`。\n",
    "\n",
    "这提供了巨大的便利并避免重复造轮子的样板代码。\n",
    "**注：不要去在意样板代码内部的东西，就像你去接水，你只需要会打开水龙头即可，不需要知道三峡大坝是怎么造的！如果你实在想要知道，等学会开关水龙头的一切之后再说。**\n",
    "\n",
    "在本教程中，我们将使用 CIFAR10 数据集。它有以下类：“飞机”、“汽车”、“鸟”、“猫”、“鹿”、“狗”、“青蛙”、“马”、“船”、“卡车”。CIFAR-10 中的图像大小为 3x32x32，即 32x32 像素大小的 3 通道彩色图像。\n",
    "\n",
    "[cifar10](https://www.hualigs.cn/image/6182ab6e296f4.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 训练图像分类器\n",
    "我们将按照如下步骤顺序执行：\n",
    "1. 使用以下命令加载和标准化CIFAR10训练和测试数据集`torchvision`\n",
    "2. 定义卷积神经网络\n",
    "3. 定义止损函数\n",
    "4. 在训练数据上训练网络。\n",
    "5. 在测试数据上评估网络。\n",
    "\n",
    "### 6.2.1 加载并标准化CIFAR10\n",
    "`torchvision`数据集的输出是范围`[0,1]`的PILImage图像。我们将他们转换为归一化范围`[-1, 1]`的张量\n",
    "由于heywhale的同学们已经将数据集传输到了平台，这里直接挂载相关数据即可，挂载方法如下：\n",
    "\n",
    "[如何挂载](https://www.hualigs.cn/image/618380c4e9afe.jpg)\n",
    "\n",
    "**注意**：最好是选择箭头指的那一个，否则挂载的`tar.gz`还需要解压，还有一定要修改一下文件名\n",
    "```shell\n",
    "cd /home/mv/input\n",
    "mv /home/mw/input/cifa10_pytorch99921 ./cifar-10-batches-py\n",
    "```\n",
    "然后在下面的代码中修改路径：\n",
    "```python\n",
    "data_path = r'/home/mw/input' # 这里填入你的数据集cifar-10-batches-py所在的目录即可，\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# torchvision.transforms主要是用于常见的一些图形变换\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])\n",
    "batch_size = 4 # 一次训练所选取的样本数为4\n",
    "data_path = r'/home/mw/input'\n",
    "trainset = torchvision.datasets.CIFAR10(root=data_path, train=True,\n",
    "                                        download=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root=data_path, train=False,\n",
    "                                       download=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=0)\n",
    "classes = ('place', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了能够形象化的理解，我们展示一些图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "    plt.show()\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 定义一个卷积神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5) # 定义3个输入通道，6个数据通道，卷积核的尺寸为5\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # kernel_size 为 2，移动的步长为 2\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.3 定义一个损失函数和优化器\n",
    "\n",
    "让我们使用带有动量的分类交叉熵损失和 SGD。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.4 训练网络\n",
    "这是事情开始变得有趣的时候。我们只需要遍历我们的数据迭代器，并将输入提供给网络并进行优化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        inputs, labels = data # 获取输入、输入标签列表\n",
    "        optimizer.zero_grad() # 梯度归零\n",
    "\n",
    "        # 正向+反向+分类器\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels) # 损失函数\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999: # 每两千次训练，打印一次\n",
    "            print('{}, {:5d} loss: {:.3f}'.format(epoch+1, i+1, running_loss/2000))\n",
    "            running_loss = 0.0\n",
    "print('完成训练！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 让我们快速保存我们训练好的模型：\n",
    "PATH = './cifar_net.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有关 保存 PyTorch 模型的更多详细信息，请参见[此处](http://https://pytorch.org/docs/stable/notes/serialization.html)。\n",
    "\n",
    "### 6.3.5 在测试数据上测试网络\n",
    "我们已经在训练数据集上训练了 2 遍网络。但是我们需要检查网络是否学到了任何东西。\n",
    "\n",
    "我们将通过预测神经网络输出的类标签来检查这一点，并根据真实情况进行检查。如果预测正确，我们将样本添加到正确预测列表中。\n",
    "\n",
    "好的，第一步。让我们展示一张来自测试集的图片来熟悉一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('大概识别结果：', ' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，让我们重新加载我们保存的模型（注意：这里不需要保存和重新加载模型，我们只是为了说明如何操作）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))\n",
    "outputs = net(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好的，现在让我们看看神经网络认为上面这些例子是什么： 输出是 10 个类别的**能量**。一个类别的能量越高，网络就越认为该图像属于特定类别。所以，**让我们得到最高能量的指数**："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果似乎很不错。\n",
    "\n",
    "让我们看看网络在整个数据集上的表现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# 本次不是为了训练，所以不用计算输出的梯度值\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images) # 通过运行经过网络的images计算结果\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0) # 统计所有标签作为分母\n",
    "        correct += (predicted == labels).sum().item() # 统计预测值与标签值一致的个数\n",
    "print('在10000张图片里面运行测试集的准确性为：%d %%' %(100 * correct / total)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这看起来比偶然性好得多，它是 10% 的准确率（从 10 个类中随机选择一个类）。似乎神经网络学到了一些东西。\n",
    "接下来我们看看哪些分类表现良好，哪些分类表现不佳："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # 计算每个类预测正确的个数\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "# 打印每个类的准确性\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accurary = 100 * float(correct_count) / total_pred[classname]\n",
    "    print('每个类的准确性: {:5s} is {:.3f} %'.format(classname, accurary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好的，[60分钟入门的官方Pytorch教程](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)结束啦~\n",
    "\n",
    "学到这里的你，真！的！很！棒！ 能够坚持看下来，也说明你的深度学习的决心，我们一起共勉吧！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 七、总结与致谢\n",
    "非常感谢你能够读到这里，Pytorch60分钟闪电入门深度学习的课程，是很多立志成为深度学习算法工程师的必修课，也是官方、知乎、LeetCode大佬推荐的入门教程。\n",
    "\n",
    "笔者是一名数据分析工程师，想成为数据挖掘|推荐算法工程师的我，为了换行进行了多次面试但屡屡碰壁！一气之下决定系统地学习一下深度学习技术，经过多次分析与筛选之后，觉得使用Pytorch作为深度学习的平台来进行实践。\n",
    "\n",
    "为了快速了解基于Pytorch的深度学习平台，笔者就进行这个官方课程的学习，在这里我将内容完整呈现给大家，一是希望能够提供给大家一些高质量的学习渠道，二是能够和大家一起进行深度学习。\n",
    "\n",
    "最后试着对整个内容进行总结，欠妥的地方请大家海涵：\n",
    "\n",
    "第一、二章简单讲了什么是Pytorch以及相应的目的（原链接就是这么简单的描述），秉持着“字越少，事情越大”的直觉，感觉后面学习应该不止60分钟（果不其然，最后我学下来加上码字总共花了2天！）”\n",
    "\n",
    "第三章开始，就进入了主题：一开始讲了什么是张量（Tensors），讲到了`torch.from_numpy`和`t.numpy()`化的相互转换，讲到了张量的属性，张量的操作。这一章就是一些概念的堆积，如果没有numpy的知识，突然一下子接触到这些概念可能有点害怕。其实这个东西就像记人名一样，熟悉啦就记住啦，所以读者不要产生焦虑，大胆的往后看。\n",
    "\n",
    "第四章直接讲梯度，讲到了梯度的定义，Pytorch中梯度的使用，讲了损失函数，前（后）向传播，有向无环图(DAG)，顺带讲了个图像识别的resnet18算法。看的我心里一紧，对初学者来说确实不太友好，这么多的概念堆砌需要有数据结构与算法的知识铺垫。这里的学习心态是：上面这些知识点就像学游泳的每部分肢体动作，我未必要把它都学到了完美再去游泳，而是先知道他大概是个什么东西，后期再实践的过程中反复学习记忆。解决方案是：当忘了某个部分，可以回来和笔者一起反复的看，反复的对某一点来进行实践，【无他，唯手熟尔】。\n",
    "\n",
    "第五章讲到了主干，定义什么是神经网络、损失函数在神经网络中是怎么计算的，反向传播的步骤又是啥。看到这里，似乎把上一章学的肢体动作穿起来了，所以到这里就更不用害怕啦，后续无非就是将第四章的肢体动作和第五章的全身动作进行实践啦。\n",
    "\n",
    "第六章下水游泳，讲了一种图像分类器，构建了神经网络、损失函数、优化器，进行了神经网络的训练，在训练的过程中进行相应的输出，最后在测试数据集上进行测试。这里可能有很多知识点又没有学过，其实笔者在看的过程中很多地方也是一脸雾水，遇到的新的函数，不怕，跟着他练就行了，实在不懂的函数可以去百度。学习嘛，就是在披荆斩棘成长起来的。\n",
    "\n",
    "到这里，基本打通了全部章节啦，后面可以欢快地在深度学习的海洋中“溺水”啦，不管怎样，直接在平台上**干实例**吧。\n",
    "\n",
    "最后，其实后面还有一点，那就是传说中的**GPU**训练，在第三章的时候点了一下，后文中教程里面又做了一下GPU的前移，作为彩蛋选看吧~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 彩蛋 在GPU上进行训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "就像将张量转移到 GPU 上一样，您将神经网络转移到 GPU 上。\n",
    "\n",
    "如果我们有可用的 CUDA，让我们首先将我们的设备定义为第一个可见的 cuda 设备："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本节的其余部分假设它device是一个 CUDA 设备。\n",
    "\n",
    "然后这些方法将递归地遍历所有模块并将它们的参数和缓冲区转换为 CUDA 张量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请记住，您还必须在每一步将输入和目标发送到 GPU："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = data[0].to(device), data[1].to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与 CPU 相比，为什么我没有注意到 MASSIVE 加速？因为你的网络真的很小。\n",
    "\n",
    "练习：尝试增加网络的宽度（第一个的参数 2nn.Conv2d和第二个的参数 1 nn.Conv2d- 它们必须是相同的数字），看看你得到什么样的加速。\n",
    "\n",
    "实现的目标：\n",
    "\n",
    "从高层次理解 PyTorch 的张量库和神经网络。\n",
    "训练一个小型神经网络来对图像进行分类\n",
    "\n",
    "如果您想使用所有 GPU 看到更大的加速，请查看可选：[数据并行](https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html)。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
